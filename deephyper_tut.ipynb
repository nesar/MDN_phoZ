{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "going-token",
   "metadata": {},
   "source": [
    "# Deep Hyper Tutorial\n",
    "https://deephyper.readthedocs.io/en/latest/tutorials/hps_dl_basic.html\n",
    "\n",
    "We will illustrate DeepHyper HPS using a regression example. We generate synthetic data according to `y=x^T x`\n",
    " for random `N`-dimensional input vectors `x`. Our regression model is a multilayer perceptron with 1 hidden layer, implemented in Keras. Using HPS, we will then tune the model hyperparameters to optimize the validation \n",
    "`R^2` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiac-engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (8000, 10)\n",
      "train_y shape: (8000, 1)\n",
      "valid_X shape: (2000, 10)\n",
      "valid_y shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "\n",
    "def load_data(dim=10, a=-50, b=50, prop=0.80, size=10000):\n",
    "    \"\"\"Generate a random distribution of data for polynome_2 function: -SUM(X**2) where \"**\" is an element wise operator in the continuous range [a, b].\n",
    "\n",
    "    Args:\n",
    "        dim (int): size of input vector for the polynome_2 function.\n",
    "        a (int): minimum bound for all X dimensions.\n",
    "        b (int): maximum bound for all X dimensions.\n",
    "        prop (float): a value between [0., 1.] indicating how to split data between training set and validation set. `prop` corresponds to the ratio of data in training set. `1.-prop` corresponds to the amount of data in validation set.\n",
    "        size (int): amount of data to generate. It is equal to `len(training_data)+len(validation_data).\n",
    "\n",
    "    Returns:\n",
    "        tuple(tuple(ndarray, ndarray), tuple(ndarray, ndarray)): of Numpy arrays: `(train_X, train_y), (valid_X, valid_y)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def polynome_2(x):\n",
    "        return -sum([x_i ** 2 for x_i in x])\n",
    "\n",
    "    d = b - a\n",
    "    x = np.array([a + np.random.random(dim) * d for i in range(size)])\n",
    "    y = np.array([[polynome_2(v)] for v in x])\n",
    "\n",
    "    sep_index = int(prop * size)\n",
    "    train_X = x[:sep_index]\n",
    "    train_y = y[:sep_index]\n",
    "\n",
    "    valid_X = x[sep_index:]\n",
    "    valid_y = y[sep_index:]\n",
    "\n",
    "    print(f\"train_X shape: {np.shape(train_X)}\")\n",
    "    print(f\"train_y shape: {np.shape(train_y)}\")\n",
    "    print(f\"valid_X shape: {np.shape(valid_X)}\")\n",
    "    print(f\"valid_y shape: {np.shape(valid_y)}\")\n",
    "    return (train_X, train_y), (valid_X, valid_y)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consecutive-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (8000, 10)\n",
      "train_y shape: (8000, 1)\n",
      "valid_X shape: (2000, 10)\n",
      "valid_y shape: (2000, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "125/125 [==============================] - 2s 6ms/step - loss: 72685077.2698 - r2: -12.6876 - val_loss: 65497080.0000 - val_r2: -10.4848\n",
      "Epoch 2/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 59855892.4762 - r2: -10.4179 - val_loss: 47295512.0000 - val_r2: -7.3174\n",
      "Epoch 3/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 41860799.6825 - r2: -7.0476 - val_loss: 28831444.0000 - val_r2: -4.0850\n",
      "Epoch 4/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 24067963.1746 - r2: -3.5530 - val_loss: 15355092.0000 - val_r2: -1.7024\n",
      "Epoch 5/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 12350301.7381 - r2: -1.3955 - val_loss: 7665547.5000 - val_r2: -0.3420\n",
      "Epoch 6/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 6450947.1865 - r2: -0.2068 - val_loss: 5293924.0000 - val_r2: 0.0769\n",
      "Epoch 7/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 4723053.7917 - r2: 0.1099 - val_loss: 4439147.0000 - val_r2: 0.2254\n",
      "Epoch 8/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 3984870.9028 - r2: 0.2520 - val_loss: 3968278.7500 - val_r2: 0.3075\n",
      "Epoch 9/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 3695432.8075 - r2: 0.3266 - val_loss: 3669597.7500 - val_r2: 0.3596\n",
      "Epoch 10/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 3386152.4246 - r2: 0.3608 - val_loss: 3422861.7500 - val_r2: 0.4017\n",
      "Epoch 11/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 3103165.0079 - r2: 0.4102 - val_loss: 3252467.5000 - val_r2: 0.4304\n",
      "Epoch 12/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 3019880.0238 - r2: 0.4459 - val_loss: 3133837.7500 - val_r2: 0.4507\n",
      "Epoch 13/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2878873.2778 - r2: 0.4606 - val_loss: 3044270.2500 - val_r2: 0.4683\n",
      "Epoch 14/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2868702.3393 - r2: 0.4622 - val_loss: 3000353.5000 - val_r2: 0.4754\n",
      "Epoch 15/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2787878.2242 - r2: 0.4815 - val_loss: 2972828.0000 - val_r2: 0.4812\n",
      "Epoch 16/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2623591.7778 - r2: 0.4981 - val_loss: 2928671.7500 - val_r2: 0.4903\n",
      "Epoch 17/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2697284.6607 - r2: 0.4964 - val_loss: 2897084.5000 - val_r2: 0.4958\n",
      "Epoch 18/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2679562.9504 - r2: 0.4912 - val_loss: 2874372.7500 - val_r2: 0.5000\n",
      "Epoch 19/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2612611.8214 - r2: 0.5130 - val_loss: 2856836.5000 - val_r2: 0.5027\n",
      "Epoch 20/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2568017.5377 - r2: 0.5246 - val_loss: 2832755.7500 - val_r2: 0.5080\n",
      "Epoch 21/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2648991.2401 - r2: 0.5118 - val_loss: 2815822.2500 - val_r2: 0.5112\n",
      "Epoch 22/1000\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2513481.2163 - r2: 0.5231 - val_loss: 2781079.0000 - val_r2: 0.5170\n",
      "Epoch 23/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2556524.8452 - r2: 0.5351 - val_loss: 2755113.7500 - val_r2: 0.5217\n",
      "Epoch 24/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2554387.6528 - r2: 0.5326 - val_loss: 2729114.2500 - val_r2: 0.5272\n",
      "Epoch 25/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2538717.4524 - r2: 0.5157 - val_loss: 2723860.0000 - val_r2: 0.5276\n",
      "Epoch 26/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2447722.7361 - r2: 0.5381 - val_loss: 2694459.2500 - val_r2: 0.5330\n",
      "Epoch 27/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2422810.8710 - r2: 0.5376 - val_loss: 2671417.7500 - val_r2: 0.5373\n",
      "Epoch 28/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2498380.8254 - r2: 0.5354 - val_loss: 2642359.0000 - val_r2: 0.5424\n",
      "Epoch 29/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2420711.3690 - r2: 0.5362 - val_loss: 2628259.0000 - val_r2: 0.5447\n",
      "Epoch 30/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2444296.4504 - r2: 0.5519 - val_loss: 2606869.2500 - val_r2: 0.5485\n",
      "Epoch 31/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2369931.5149 - r2: 0.5516 - val_loss: 2588625.5000 - val_r2: 0.5517\n",
      "Epoch 32/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2363832.4921 - r2: 0.5547 - val_loss: 2580880.2500 - val_r2: 0.5530\n",
      "Epoch 33/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2376228.5675 - r2: 0.5454 - val_loss: 2573212.5000 - val_r2: 0.5536\n",
      "Epoch 34/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2358855.2242 - r2: 0.5625 - val_loss: 2547724.7500 - val_r2: 0.5588\n",
      "Epoch 35/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2311868.0040 - r2: 0.5706 - val_loss: 2538197.2500 - val_r2: 0.5601\n",
      "Epoch 36/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2273407.1567 - r2: 0.5733 - val_loss: 2519630.7500 - val_r2: 0.5635\n",
      "Epoch 37/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2378716.0496 - r2: 0.5757 - val_loss: 2522084.7500 - val_r2: 0.5629\n",
      "Epoch 38/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2252574.4137 - r2: 0.5609 - val_loss: 2505173.0000 - val_r2: 0.5661\n",
      "Epoch 39/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2276981.8333 - r2: 0.5719 - val_loss: 2496481.0000 - val_r2: 0.5687\n",
      "Epoch 40/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2261783.3740 - r2: 0.5694 - val_loss: 2502084.7500 - val_r2: 0.5672\n",
      "Epoch 41/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2288868.5595 - r2: 0.5692 - val_loss: 2495688.5000 - val_r2: 0.5687\n",
      "Epoch 42/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2284704.5079 - r2: 0.5822 - val_loss: 2484832.5000 - val_r2: 0.5700\n",
      "Epoch 43/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2288148.8075 - r2: 0.5738 - val_loss: 2477762.7500 - val_r2: 0.5717\n",
      "Epoch 44/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2279895.5833 - r2: 0.5688 - val_loss: 2472403.7500 - val_r2: 0.5720\n",
      "Epoch 45/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2241669.5198 - r2: 0.5768 - val_loss: 2471868.0000 - val_r2: 0.5722\n",
      "Epoch 46/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2237192.3194 - r2: 0.5830 - val_loss: 2468841.2500 - val_r2: 0.5733\n",
      "Epoch 47/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2294217.4405 - r2: 0.5780 - val_loss: 2465670.2500 - val_r2: 0.5733\n",
      "Epoch 48/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2259450.0476 - r2: 0.5768 - val_loss: 2469408.5000 - val_r2: 0.5728\n",
      "Epoch 49/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2259837.0238 - r2: 0.5839 - val_loss: 2459689.5000 - val_r2: 0.5742\n",
      "Epoch 50/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2245652.1488 - r2: 0.5801 - val_loss: 2463175.2500 - val_r2: 0.5741\n",
      "Epoch 51/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2266189.6389 - r2: 0.5810 - val_loss: 2454990.2500 - val_r2: 0.5757\n",
      "Epoch 52/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2216628.4881 - r2: 0.5864 - val_loss: 2448763.5000 - val_r2: 0.5764\n",
      "Epoch 53/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2237393.4960 - r2: 0.5789 - val_loss: 2446241.5000 - val_r2: 0.5775\n",
      "Epoch 54/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2231593.2956 - r2: 0.5835 - val_loss: 2445607.5000 - val_r2: 0.5780\n",
      "Epoch 55/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2164837.5060 - r2: 0.5886 - val_loss: 2447086.0000 - val_r2: 0.5778\n",
      "Epoch 56/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2172204.6359 - r2: 0.6001 - val_loss: 2439793.5000 - val_r2: 0.5784\n",
      "Epoch 57/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2173015.6448 - r2: 0.5725 - val_loss: 2435309.7500 - val_r2: 0.5792\n",
      "Epoch 58/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2220228.2579 - r2: 0.5809 - val_loss: 2436516.0000 - val_r2: 0.5790\n",
      "Epoch 59/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2160615.9216 - r2: 0.5914 - val_loss: 2437038.0000 - val_r2: 0.5795\n",
      "Epoch 60/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2246662.8532 - r2: 0.5939 - val_loss: 2422279.0000 - val_r2: 0.5823\n",
      "Epoch 61/1000\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2135984.4494 - r2: 0.5948 - val_loss: 2431558.5000 - val_r2: 0.5805\n",
      "Epoch 62/1000\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2189651.1875 - r2: 0.5838 - val_loss: 2424427.0000 - val_r2: 0.5813\n",
      "Epoch 63/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2202765.4405 - r2: 0.5820 - val_loss: 2414434.7500 - val_r2: 0.5831\n",
      "Epoch 64/1000\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2181299.6905 - r2: 0.5995 - val_loss: 2419978.2500 - val_r2: 0.5825\n",
      "Epoch 65/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2161234.0060 - r2: 0.5973 - val_loss: 2403330.2500 - val_r2: 0.5855\n",
      "Epoch 66/1000\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2106061.3294 - r2: 0.5874 - val_loss: 2389541.0000 - val_r2: 0.5879\n",
      "Epoch 67/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2102517.2014 - r2: 0.5939 - val_loss: 2387266.2500 - val_r2: 0.5881\n",
      "Epoch 68/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2082818.6409 - r2: 0.6059 - val_loss: 2375584.7500 - val_r2: 0.5901\n",
      "Epoch 69/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2106005.9107 - r2: 0.5993 - val_loss: 2372618.5000 - val_r2: 0.5901\n",
      "Epoch 70/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2152068.8135 - r2: 0.5837 - val_loss: 2368777.7500 - val_r2: 0.5911\n",
      "Epoch 71/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2191156.6875 - r2: 0.5885 - val_loss: 2358027.0000 - val_r2: 0.5928\n",
      "Epoch 72/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2060889.2599 - r2: 0.6103 - val_loss: 2354446.5000 - val_r2: 0.5933\n",
      "Epoch 73/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2128716.6071 - r2: 0.6055 - val_loss: 2348099.2500 - val_r2: 0.5945\n",
      "Epoch 74/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2098828.7093 - r2: 0.6124 - val_loss: 2334772.5000 - val_r2: 0.5961\n",
      "Epoch 75/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2077524.1260 - r2: 0.6054 - val_loss: 2329355.2500 - val_r2: 0.5967\n",
      "Epoch 76/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2093469.5159 - r2: 0.6132 - val_loss: 2330183.2500 - val_r2: 0.5958\n",
      "Epoch 77/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2075881.2728 - r2: 0.6009 - val_loss: 2320038.5000 - val_r2: 0.5970\n",
      "Epoch 78/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2097459.5972 - r2: 0.5963 - val_loss: 2313584.2500 - val_r2: 0.5977\n",
      "Epoch 79/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2077024.5804 - r2: 0.6107 - val_loss: 2308505.2500 - val_r2: 0.5993\n",
      "Epoch 80/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2019867.2976 - r2: 0.6116 - val_loss: 2299816.0000 - val_r2: 0.5999\n",
      "Epoch 81/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2056264.3313 - r2: 0.6109 - val_loss: 2289987.2500 - val_r2: 0.6009\n",
      "Epoch 82/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2041731.8562 - r2: 0.6155 - val_loss: 2286827.0000 - val_r2: 0.6015\n",
      "Epoch 83/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2035012.2837 - r2: 0.6157 - val_loss: 2287405.0000 - val_r2: 0.6003\n",
      "Epoch 84/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2065781.0923 - r2: 0.6139 - val_loss: 2280464.2500 - val_r2: 0.6014\n",
      "Epoch 85/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2051442.5218 - r2: 0.6200 - val_loss: 2272888.0000 - val_r2: 0.6030\n",
      "Epoch 86/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2115114.3512 - r2: 0.6133 - val_loss: 2265091.5000 - val_r2: 0.6042\n",
      "Epoch 87/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2065479.8512 - r2: 0.6066 - val_loss: 2259683.0000 - val_r2: 0.6043\n",
      "Epoch 88/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2037680.7351 - r2: 0.6136 - val_loss: 2256483.7500 - val_r2: 0.6047\n",
      "Epoch 89/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2106263.1349 - r2: 0.6098 - val_loss: 2253437.5000 - val_r2: 0.6050\n",
      "Epoch 90/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2028758.7460 - r2: 0.6168 - val_loss: 2251485.5000 - val_r2: 0.6049\n",
      "Epoch 91/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2064257.6032 - r2: 0.6185 - val_loss: 2241183.0000 - val_r2: 0.6071\n",
      "Epoch 92/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1993352.7450 - r2: 0.6142 - val_loss: 2235890.0000 - val_r2: 0.6073\n",
      "Epoch 93/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2065995.7579 - r2: 0.6158 - val_loss: 2228607.0000 - val_r2: 0.6086\n",
      "Epoch 94/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1996288.4365 - r2: 0.6248 - val_loss: 2225234.7500 - val_r2: 0.6087\n",
      "Epoch 95/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2053944.3264 - r2: 0.6137 - val_loss: 2225795.5000 - val_r2: 0.6075\n",
      "Epoch 96/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1995398.2847 - r2: 0.6234 - val_loss: 2214187.0000 - val_r2: 0.6103\n",
      "Epoch 97/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2019176.8542 - r2: 0.6183 - val_loss: 2214172.7500 - val_r2: 0.6113\n",
      "Epoch 98/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2000478.2192 - r2: 0.6168 - val_loss: 2203417.0000 - val_r2: 0.6117\n",
      "Epoch 99/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1943176.8095 - r2: 0.6275 - val_loss: 2202462.2500 - val_r2: 0.6119\n",
      "Epoch 100/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2035402.0208 - r2: 0.6115 - val_loss: 2199317.0000 - val_r2: 0.6126\n",
      "Epoch 101/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2006840.0248 - r2: 0.6200 - val_loss: 2194161.0000 - val_r2: 0.6136\n",
      "Epoch 102/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1983809.7550 - r2: 0.6254 - val_loss: 2200405.7500 - val_r2: 0.6112\n",
      "Epoch 103/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2017309.4385 - r2: 0.6262 - val_loss: 2190612.2500 - val_r2: 0.6134\n",
      "Epoch 104/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1948500.6528 - r2: 0.6361 - val_loss: 2188314.2500 - val_r2: 0.6144\n",
      "Epoch 105/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2028277.7758 - r2: 0.6284 - val_loss: 2192166.5000 - val_r2: 0.6133\n",
      "Epoch 106/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1956516.6528 - r2: 0.6293 - val_loss: 2177790.2500 - val_r2: 0.6152\n",
      "Epoch 107/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1949057.2956 - r2: 0.6240 - val_loss: 2180486.5000 - val_r2: 0.6139\n",
      "Epoch 108/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2006139.9742 - r2: 0.6320 - val_loss: 2181570.0000 - val_r2: 0.6132\n",
      "Epoch 109/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1967182.9196 - r2: 0.6288 - val_loss: 2174678.5000 - val_r2: 0.6157\n",
      "Epoch 110/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2024479.7242 - r2: 0.6145 - val_loss: 2181703.5000 - val_r2: 0.6133\n",
      "Epoch 111/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2017158.3611 - r2: 0.6275 - val_loss: 2174768.2500 - val_r2: 0.6143\n",
      "Epoch 112/1000\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2033529.8601 - r2: 0.6081 - val_loss: 2173720.7500 - val_r2: 0.6152\n",
      "Epoch 113/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1960230.5129 - r2: 0.6257 - val_loss: 2170877.7500 - val_r2: 0.6155\n",
      "Epoch 114/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1976208.0982 - r2: 0.6330 - val_loss: 2173518.5000 - val_r2: 0.6125\n",
      "Epoch 115/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1991514.8581 - r2: 0.6250 - val_loss: 2168014.0000 - val_r2: 0.6154\n",
      "Epoch 116/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1984127.3671 - r2: 0.6274 - val_loss: 2165378.2500 - val_r2: 0.6151\n",
      "Epoch 117/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1969451.6696 - r2: 0.6283 - val_loss: 2169783.5000 - val_r2: 0.6138\n",
      "Epoch 118/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1968980.2431 - r2: 0.6295 - val_loss: 2162087.0000 - val_r2: 0.6158\n",
      "Epoch 119/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2001357.9712 - r2: 0.6324 - val_loss: 2162240.2500 - val_r2: 0.6160\n",
      "Epoch 120/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1925688.6339 - r2: 0.6393 - val_loss: 2172668.5000 - val_r2: 0.6137\n",
      "Epoch 121/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1950142.2847 - r2: 0.6243 - val_loss: 2165292.0000 - val_r2: 0.6155\n",
      "Epoch 122/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2002272.0714 - r2: 0.6309 - val_loss: 2161473.0000 - val_r2: 0.6166\n",
      "Epoch 123/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1981098.6736 - r2: 0.6261 - val_loss: 2165419.0000 - val_r2: 0.6149\n",
      "Epoch 124/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1997890.3790 - r2: 0.6219 - val_loss: 2168624.7500 - val_r2: 0.6146\n",
      "Epoch 125/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2015530.6706 - r2: 0.6290 - val_loss: 2162024.0000 - val_r2: 0.6155\n",
      "Epoch 126/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1929755.6587 - r2: 0.6407 - val_loss: 2166749.2500 - val_r2: 0.6152\n",
      "Epoch 127/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1944497.9921 - r2: 0.6319 - val_loss: 2161294.0000 - val_r2: 0.6150\n",
      "Epoch 128/1000\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1944605.5575 - r2: 0.6232 - val_loss: 2161379.7500 - val_r2: 0.6156\n",
      "Epoch 129/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2003956.1220 - r2: 0.6247 - val_loss: 2157472.0000 - val_r2: 0.6161\n",
      "Epoch 130/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1949537.1944 - r2: 0.6241 - val_loss: 2157029.0000 - val_r2: 0.6159\n",
      "Epoch 131/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1925155.5060 - r2: 0.6378 - val_loss: 2170820.2500 - val_r2: 0.6131\n",
      "Epoch 132/1000\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1992710.9077 - r2: 0.6244 - val_loss: 2165407.5000 - val_r2: 0.6143\n",
      "Epoch 00132: early stopping\n",
      "objective:  0.6142663359642029\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "here = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.insert(0, here)\n",
    "\n",
    "\n",
    "def r2(y_true, y_pred): # Like loss function\n",
    "    SS_res = keras.backend.sum(keras.backend.square(y_true - y_pred), axis=0)\n",
    "    SS_tot = keras.backend.sum(\n",
    "        keras.backend.square(y_true - keras.backend.mean(y_true, axis=0)), axis=0\n",
    "    )\n",
    "    output_scores = 1 - SS_res / (SS_tot + keras.backend.epsilon())\n",
    "    r2 = keras.backend.mean(output_scores)\n",
    "    return r2\n",
    "\n",
    "\n",
    "HISTORY = None\n",
    "\n",
    "\n",
    "def run(point):\n",
    "    \"\"\"\n",
    "    point: dictionary of tunable parameters\n",
    "    \"\"\"\n",
    "    global HISTORY\n",
    "    (x_train, y_train), (x_valid, y_valid) = load_data()\n",
    "    \n",
    "    if point[\"activation\"] == \"identity\":\n",
    "        point[\"activation\"] = None\n",
    "    \t\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(\n",
    "            point[\"units\"],\n",
    "            activation=point[\"activation\"],\n",
    "            input_shape=tuple(np.shape(x_train)[1:]),\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=point[\"lr\"]), metrics=[r2])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=64,\n",
    "        epochs=1000,\n",
    "        verbose=1,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_r2\", mode=\"max\", verbose=1, patience=10)],\n",
    "        validation_data=(x_valid, y_valid),\n",
    "    )\n",
    "\n",
    "    HISTORY = history.history\n",
    "\n",
    "    return history.history[\"val_r2\"][-1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    point = {\"units\": 10, \"activation\": \"relu\", \"lr\": 0.01}\n",
    "    objective = run(point)\n",
    "    print(\"objective: \", objective)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(HISTORY[\"val_r2\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Objective: $R^2$\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "# What determines the objective? Looks kind of random... not the largest or the smallest\n",
    "# Also what is dense and (Dense)?\n",
    "# Also don't really understand this history.history thing... is it running history twice somehow? Or running history on history?\n",
    "# Why is my loss function 4 magnitudes smaller than the example in the tutorial?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-module",
   "metadata": {},
   "source": [
    "## Defining the HPS Problem Space\n",
    "The `run` function in `model_run.py` expects a hyperparameter dictionary with three keys: `units`, `activation`, and `lr`. We define the acceptable ranges for these hyperparameters with the Problem object inside `problem.py`. Hyperparameter ranges are defined using the following syntax:\n",
    "\n",
    "Discrete integer ranges are generated from a tuple: `(lower: int, upper: int)`\n",
    "\n",
    "Continous parameters are generated from a tuple: `(lower: float, upper: float)`\n",
    "\n",
    "Categorical or nonordinal hyperparameters ranges can be given as a list of possible values: `[val1, val2, ...]`\n",
    "\n",
    "You probably have one or more “reference” sets of hyperparameters that are either hand-crafted or chosen by intuition. To bootstrap the search with these so-called starting points, use the `add_starting_point(...)` method.\n",
    "\n",
    "*what do they mean \"bootstrap\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "flush-malpractice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration space object:\n",
      "  Hyperparameters:\n",
      "    activation, Type: Categorical, Choices: {identity, relu, sigmoid, tanh}, Default: identity\n",
      "    lr, Type: UniformFloat, Range: [0.0001, 1.0], Default: 0.50005\n",
      "    units, Type: UniformInteger, Range: [1, 100], Default: 50\n",
      "\n",
      "\n",
      "  Starting Point:\n",
      "{0: {'activation': 'identity', 'lr': 0.01, 'units': 10}}\n"
     ]
    }
   ],
   "source": [
    "from deephyper.problem import HpProblem\n",
    "\n",
    "Problem = HpProblem()\n",
    "\n",
    "# Like .add_argument in parsing\n",
    "Problem.add_hyperparameter((1, 100), \"units\")\n",
    "Problem.add_hyperparameter([\"identity\", \"relu\", \"sigmoid\", \"tanh\"], \"activation\")\n",
    "Problem.add_hyperparameter((0.0001, 1.0), \"lr\")\n",
    "\n",
    "Problem.add_starting_point(units=10, activation=\"identity\", lr=0.01)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "protective-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'deephyper.problem.hyperparameter.HpProblem'>\n"
     ]
    }
   ],
   "source": [
    "print(type(Problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-norwegian",
   "metadata": {},
   "source": [
    "*we don't know what asynchronous model search means*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unique-personality",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-212eed39b8b3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-212eed39b8b3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    deephyper hps ambs --problem hps_demo.polynome2.problem.Problem --run hps_demo.polynome2.model_run.run\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Not sure how to run this line from a jupyter notebook?\n",
    "deephyper hps ambs --problem hps_demo.polynome2.problem.Problem --run hps_demo.polynome2.model_run.run\n",
    "Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-announcement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
